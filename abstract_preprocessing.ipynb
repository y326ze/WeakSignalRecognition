{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "seeds = pd.read_csv('2024/seeds_2024.csv')\n",
    "references = pd.read_csv('2024/references_2024.csv')\n",
    "citations = pd.read_csv('2024/citations_2024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568, 7962, 1210)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seeds), len(references), len(citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 去除HTML标签和特殊字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义替换函数\n",
    "def standardize_punctuation(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # 使用字典定义所有需要替换的字符\n",
    "    replacements = {\n",
    "        '–': '-', '—': '-',   # 破折号\n",
    "        '“': '\"', '”': '\"',   # 双引号\n",
    "        '‘': \"'\", '’': \"'\",   # 单引号\n",
    "        '…': '...',           # 省略号\n",
    "        '《': '\"', '》': '\"',    # 法语引号\n",
    "        '（': '(', '）': ')',\n",
    "        '【': '[', '】': ']',\n",
    "        '；': ';', '：': ':',\n",
    "        '？': '?', '！': '!',\n",
    "        '，': ',', '。': '.'\n",
    "    }\n",
    "    \n",
    "    # 逐个替换\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用替换到指定列\n",
    "seeds['processed_abstract'] = seeds['abstract'].apply(standardize_punctuation)\n",
    "references['processed_abstract'] = references['abstract'].apply(standardize_punctuation)\n",
    "citations['processed_abstract'] = citations['abstract'].apply(standardize_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re, html\n",
    "\n",
    "def advanced_clean_text(text):\n",
    "    \"\"\"\n",
    "    增强版文本清理，处理更多边缘情况\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 处理常见的\"垃圾\"Unicode字符\n",
    "    text = re.sub(\n",
    "        r'[\\x00-\\x1f\\x7f-\\x9f\\u2000-\\u200f\\u2028-\\u202f\\u205f-\\u206f\\ufeff\\ufff0-\\uffff]',\n",
    "        ' ', \n",
    "        text\n",
    "    )\n",
    "    \n",
    "    # 处理HTML实体和标签\n",
    "    text = html.unescape(text)\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text(\" \")\n",
    "    \n",
    "    # 处理URL(可选)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # 处理特殊字符但保留有意义的标点\n",
    "    text = re.sub(r\"\"\"[^\\w\\s@#\\$%&\\*\\+-=<>/\\\\:;'\"\\.,\\?\\!\\(\\)\\[\\]\\{\\}]\"\"\", '', text)\n",
    "    \n",
    "    # 规范化空白\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_46308\\1872739503.py:20: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text(\" \")\n"
     ]
    }
   ],
   "source": [
    "# 应用替换到指定列\n",
    "seeds['processed_abstract'] = seeds['processed_abstract'].apply(advanced_clean_text)\n",
    "references['processed_abstract'] = references['processed_abstract'].apply(advanced_clean_text)\n",
    "citations['processed_abstract'] = citations['processed_abstract'].apply(advanced_clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 分词处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_abstracts_df(df, text_column='processed_abstract'):\n",
    "    \"\"\"\n",
    "    处理DataFrame中指定文本列的分词\n",
    "    \n",
    "    参数:\n",
    "        df: pandas DataFrame\n",
    "        text_column: 要处理的文本列名(默认为'abstract')\n",
    "    \n",
    "    返回:\n",
    "        包含分词结果的新Series\n",
    "    \"\"\"\n",
    "    return df[text_column].apply(lambda text: [token.text for token in nlp(text)]), df[text_column].apply(lambda text: [token.tag_ for token in nlp(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用替换到指定列\n",
    "seeds['tokenized_abstract'], seeds['tokenized_tag'] = tokenize_abstracts_df(seeds)\n",
    "references['tokenized_abstract'], references['tokenized_tag'] = tokenize_abstracts_df(references)\n",
    "citations['tokenized_abstract'], citations['tokenized_tag'] = tokenize_abstracts_df(citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 词形还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_wordnet_pos(spacy_tag):\n",
    "    \"\"\"将 spaCy 的词性标签转换为 WordNet 的词性标签\"\"\"\n",
    "    if spacy_tag.startswith('J'):  # 形容词\n",
    "        return wordnet.ADJ\n",
    "    elif spacy_tag.startswith('V'):  # 动词\n",
    "        return wordnet.VERB\n",
    "    elif spacy_tag.startswith('N'):  # 名词\n",
    "        return wordnet.NOUN\n",
    "    elif spacy_tag.startswith('R'):  # 副词\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # 默认作为名词处理\n",
    "    \n",
    "def lemmatize_tokens(row):\n",
    "    \"\"\"\n",
    "    使用现有的 abstract 和 tag 列进行词形还原\n",
    "    :param row: DataFrame 的一行，包含 'abstract' 和 'tag' 列\n",
    "    :return: 词形还原后的单词列表\n",
    "    \"\"\"\n",
    "    tokens = row['tokenized_abstract']\n",
    "    tags = row['tokenized_tag']\n",
    "    \n",
    "    lemmatized_tokens = []\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        wordnet_pos = get_wordnet_pos(tag)\n",
    "        lemma = lemmatizer.lemmatize(token, pos=wordnet_pos)\n",
    "        lemmatized_tokens.append(lemma.lower()) # 小写处理\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用替换到指定列\n",
    "seeds['lemmatized_abstract'] = seeds.apply(lemmatize_tokens, axis=1)\n",
    "references['lemmatized_abstract'] = references.apply(lemmatize_tokens, axis=1)\n",
    "citations['lemmatized_abstract'] = citations.apply(lemmatize_tokens, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# 确保NLTK停用词已下载（如果没有，运行第一次时会自动下载）\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 定义停用词\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 扩展领域词汇（AI + Finance）\n",
    "domain_words = {\n",
    "    # 通用AI术语\n",
    "    'ai', 'artificial', 'intelligence', 'machine', 'learning', 'ml', 'deep', 'learning', 'dl',\n",
    "    'neural', 'network', 'nn', 'transformer', 'attention', 'gpt', 'llm', 'bert', 'gpt-3', 'gpt-4',\n",
    "    'nlp', 'natural', 'language', 'processing', 'computer', 'vision', 'cv', 'generative', 'gan',\n",
    "    'reinforcement', 'learning', 'rl', 'supervised', 'unsupervised', 'semi-supervised',\n",
    "    \n",
    "    # 金融科技（FinTech）相关\n",
    "    'finance', 'financial', 'fintech', 'banking', 'investment', 'stock', 'market', 'trading',\n",
    "    'portfolio', 'risk', 'management', 'credit', 'loan', 'blockchain', 'crypto', 'cryptocurrency',\n",
    "    'bitcoin', 'ethereum', 'defi', 'algorithmic', 'forex', 'quantitative', 'hedge', 'fund',\n",
    "    'robo-advisor', 'fraud', 'detection', 'regulation', 'compliance', 'insurtech', 'payments',\n",
    "    'wealth', 'management', 'asset', 'lending', 'mortgage', 'insurance', 'audit', 'accounting',\n",
    "    'financial', 'inclusion', 'microfinance', 'crowdfunding', 'p2p', 'peer-to-peer',\n",
    "    \n",
    "    # 模型/方法相关\n",
    "    'gcn', 'graph', 'convolutional', 'network', 'lstm', 'cnn', 'rnn', 'transformer', 'attention',\n",
    "    'svm', 'random', 'forest', 'xgboost', 'clustering', 'regression', 'classification',\n",
    "    'optimization', 'bayesian', 'forecasting', 'prediction', 'anomaly', 'detection'\n",
    "}\n",
    "\n",
    "def filter_tokens(df, text_columns='lemmatized_abstract'):\n",
    "    \"\"\"\n",
    "    过滤停用词，但保留领域词汇\n",
    "    \n",
    "    参数:\n",
    "        tokenized_abstracts: 分词后的数据集（List[List[str]]）\n",
    "    \n",
    "    返回:\n",
    "        过滤后的分词结果（List[List[str]]）\n",
    "    \"\"\"\n",
    "    filtered_abstracts = []\n",
    "    for tokens in df[text_columns]:\n",
    "        filtered = [\n",
    "            t.lower() for t in tokens \n",
    "            if (t.lower() not in stop_words) or (t.lower() in domain_words)\n",
    "        ]\n",
    "        filtered_abstracts.append(filtered)\n",
    "    return filtered_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用替换到指定列\n",
    "seeds['final_abstract'] = filter_tokens(seeds)\n",
    "references['final_abstract'] = filter_tokens(references)\n",
    "citations['final_abstract'] = filter_tokens(citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存为新的CSV文件（使用UTF-8编码）\n",
    "seeds.to_csv('2024/seeds_processed_abstract_2024.csv', index=False, encoding='utf-8-sig')\n",
    "references.to_csv('2024/references_processed_abstract_2024.csv', index=False, encoding='utf-8-sig')\n",
    "citations.to_csv('2024/citations_processed_abstract_2024.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
