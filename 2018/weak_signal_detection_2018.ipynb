{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d74c0573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_clusters</th>\n",
       "      <th>diversity</th>\n",
       "      <th>coherence</th>\n",
       "      <th>cohenrences</th>\n",
       "      <th>topics_words</th>\n",
       "      <th>topic_ctfidfs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.349166</td>\n",
       "      <td>[0.4114547301956549, 0.286876395809927]</td>\n",
       "      <td>[['system', 'aceh', 'financial', 'design', 'mo...</td>\n",
       "      <td>[[164.73593709306485, 129.05863476260959, 127....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.351525</td>\n",
       "      <td>[0.423030387012301, 0.344667325177772, 0.28687...</td>\n",
       "      <td>[['system', 'aceh', 'design', 'financial', 'mh...</td>\n",
       "      <td>[[150.00345491401026, 129.05863476260959, 113....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.491591</td>\n",
       "      <td>[0.423030387012301, 0.344667325177772, 0.77871...</td>\n",
       "      <td>[['system', 'aceh', 'design', 'financial', 'mh...</td>\n",
       "      <td>[[150.00345491401026, 129.05863476260959, 113....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_clusters  diversity  coherence  \\\n",
       "0           2      0.864   0.349166   \n",
       "1           3      0.822   0.351525   \n",
       "2           4      0.788   0.491591   \n",
       "\n",
       "                                         cohenrences  \\\n",
       "0            [0.4114547301956549, 0.286876395809927]   \n",
       "1  [0.423030387012301, 0.344667325177772, 0.28687...   \n",
       "2  [0.423030387012301, 0.344667325177772, 0.77871...   \n",
       "\n",
       "                                        topics_words  \\\n",
       "0  [['system', 'aceh', 'financial', 'design', 'mo...   \n",
       "1  [['system', 'aceh', 'design', 'financial', 'mh...   \n",
       "2  [['system', 'aceh', 'design', 'financial', 'mh...   \n",
       "\n",
       "                                       topic_ctfidfs  \n",
       "0  [[164.73593709306485, 129.05863476260959, 127....  \n",
       "1  [[150.00345491401026, 129.05863476260959, 113....  \n",
       "2  [[150.00345491401026, 129.05863476260959, 113....  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "results = pd.read_csv('dgcn_clustering_results_2018.csv')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98577323",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_row = 2\n",
    "words = pd.DataFrame(eval(results.topics_words[best_row]))\n",
    "ctfidfs = pd.DataFrame(eval(results.topic_ctfidfs[best_row]))\n",
    "coherences = eval(results.cohenrences[best_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9cdc808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 统计每个主题的词频分布\n",
    "def get_topic_word_distribution(topic_words):\n",
    "    \"\"\"计算一个主题的词频分布（概率标准化）\"\"\"\n",
    "    word_counts = Counter(topic_words)\n",
    "    total_words = sum(word_counts.values())\n",
    "    return {word: count / total_words for word, count in word_counts.items()}\n",
    "\n",
    "# 获取所有主题的词频分布\n",
    "topic_dists = []\n",
    "for i in range(len(words)):\n",
    "    topic_words = words.iloc[i].dropna().tolist()  # 提取词汇并去除NaN（如果有）\n",
    "    topic_dist = get_topic_word_distribution(topic_words)\n",
    "    topic_dists.append(topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e14598d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 计算紧密中心度(Closeness Centrality)\n",
    "\n",
    "def hellinger_distance(dist1, dist2, words=words):\n",
    "    \"\"\"计算两个词频分布之间的Hellinger距离\"\"\"\n",
    "    all_words = set(dist1.keys()).union(set(dist2.keys()))\n",
    "    sqrt_p = np.array([np.sqrt(dist1.get(word, 0)) for word in all_words])\n",
    "    sqrt_q = np.array([np.sqrt(dist2.get(word, 0)) for word in all_words])\n",
    "    return np.sqrt(np.sum((sqrt_p - sqrt_q) ** 2)) / np.sqrt(2)\n",
    "\n",
    "# 计算所有主题之间的Hellinger距离矩阵\n",
    "distance_matrix = np.zeros((len(words), len(words)))\n",
    "for i in range(len(words)):\n",
    "    for j in range(len(words)):\n",
    "        distance_matrix[i, j] = hellinger_distance(topic_dists[i], topic_dists[j])\n",
    "\n",
    "# 计算紧密中心度\n",
    "closeness_centrality = {}\n",
    "for t in range(len(words)):\n",
    "    sum_distances = np.sum(distance_matrix[t])  # 计算主题t到所有其他主题的距离和\n",
    "    closeness_centrality[t] = 1 / sum_distances if sum_distances != 0 else 0\n",
    "\n",
    "# 计算主题权重(Topic Weight)\n",
    "\n",
    "total_coherence = sum(coherences)\n",
    "topic_weights = {t: coh / total_coherence for t, coh in enumerate(coherences)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a801154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到的主题: [2, 0, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "with open('dgcn_result_2018.pkl', 'rb') as f:\n",
    "        G = pickle.load(f)\n",
    "\n",
    "# 提取所有 seeds 节点（文档）\n",
    "seeds_nodes = [n for n in G.nodes if G.nodes[n]['type'] == 'seed']\n",
    "\n",
    "# 按主题分组文档\n",
    "topic_docs = defaultdict(list)\n",
    "for node in seeds_nodes:\n",
    "    topic = G.nodes[node]['cluster']\n",
    "    text = G.nodes[node]['bert_abstract']  # 文档内容\n",
    "    topic_docs[topic].append(text)\n",
    "\n",
    "# 检查主题数量\n",
    "topics = list(topic_docs.keys())\n",
    "print(f\"找到的主题: {topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f99468af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题自相关性 AC(t): {2: 0.14716305458933954, 0: 0.01551340066637781, 3: 0.22594441734252083, 1: 0.06597545022311942}\n"
     ]
    }
   ],
   "source": [
    "# 构建所有主题的共享词汇表\n",
    "all_words = list(set([w for i in range(len(words)) for w in words.iloc[i]]))\n",
    "word_to_idx = {word: i for i, word in enumerate(all_words)}\n",
    "\n",
    "# 初始化协方差矩阵存储\n",
    "topic_ac_scores = {}\n",
    "\n",
    "for topic in topics:\n",
    "    docs = topic_docs[topic]\n",
    "    # 统计高频词在文档中的出现次数（二进制表示是否出现）\n",
    "    word_occurrence = np.zeros((len(docs), len(all_words)))\n",
    "    for i, doc in enumerate(docs):\n",
    "        for word in doc.split():\n",
    "            if word in word_to_idx:\n",
    "                word_occurrence[i, word_to_idx[word]] = 1\n",
    "    # 计算词汇协方差矩阵\n",
    "    cov_matrix = np.cov(word_occurrence, rowvar=False)  # 变量是词汇，所以 rowvar=False\n",
    "    # 计算均值和方差\n",
    "    mean_cov = np.mean(cov_matrix)\n",
    "    var_t = np.var(word_occurrence)\n",
    "    # 避免除以零\n",
    "    topic_ac_scores[topic] = mean_cov / var_t if var_t != 0 else 0\n",
    "\n",
    "print(\"主题自相关性 AC(t):\", topic_ac_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0ef3e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Docs</th>\n",
       "      <th>Words</th>\n",
       "      <th>Ctfidfs</th>\n",
       "      <th>Closeness_Centrality</th>\n",
       "      <th>Topic_Weight</th>\n",
       "      <th>Auto_Correleration</th>\n",
       "      <th>Weakness</th>\n",
       "      <th>Weakness_Min_Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[intensification globalization rapid developme...</td>\n",
       "      <td>[biometric, information, new, dt, model, segre...</td>\n",
       "      <td>[52.4487602698934, 45.8059798376516, 37.724831...</td>\n",
       "      <td>0.369410</td>\n",
       "      <td>0.175282</td>\n",
       "      <td>0.015513</td>\n",
       "      <td>0.042072</td>\n",
       "      <td>0.198547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[advent artificial intelligence way technology...</td>\n",
       "      <td>[food, cloud, iot, plastic, intelligence, data...</td>\n",
       "      <td>[116.73559036766228, 99.09377689622501, 91.611...</td>\n",
       "      <td>0.381418</td>\n",
       "      <td>0.213565</td>\n",
       "      <td>0.065975</td>\n",
       "      <td>0.080199</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[specialty grand challenge article front artif...</td>\n",
       "      <td>[system, aceh, design, financial, mhpp, contra...</td>\n",
       "      <td>[150.00345491401026, 129.05863476260959, 113.0...</td>\n",
       "      <td>0.374150</td>\n",
       "      <td>0.215133</td>\n",
       "      <td>0.147163</td>\n",
       "      <td>0.032627</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[advances artificial intelligence ( ai ) trans...</td>\n",
       "      <td>[art, infertility, child, health, law, might, ...</td>\n",
       "      <td>[79.54152561568326, 69.39018841521411, 64.1453...</td>\n",
       "      <td>0.364071</td>\n",
       "      <td>0.396020</td>\n",
       "      <td>0.225944</td>\n",
       "      <td>0.043202</td>\n",
       "      <td>0.222303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic                                               Docs  \\\n",
       "0      0  [intensification globalization rapid developme...   \n",
       "1      1  [advent artificial intelligence way technology...   \n",
       "2      2  [specialty grand challenge article front artif...   \n",
       "3      3  [advances artificial intelligence ( ai ) trans...   \n",
       "\n",
       "                                               Words  \\\n",
       "0  [biometric, information, new, dt, model, segre...   \n",
       "1  [food, cloud, iot, plastic, intelligence, data...   \n",
       "2  [system, aceh, design, financial, mhpp, contra...   \n",
       "3  [art, infertility, child, health, law, might, ...   \n",
       "\n",
       "                                             Ctfidfs  Closeness_Centrality  \\\n",
       "0  [52.4487602698934, 45.8059798376516, 37.724831...              0.369410   \n",
       "1  [116.73559036766228, 99.09377689622501, 91.611...              0.381418   \n",
       "2  [150.00345491401026, 129.05863476260959, 113.0...              0.374150   \n",
       "3  [79.54152561568326, 69.39018841521411, 64.1453...              0.364071   \n",
       "\n",
       "   Topic_Weight  Auto_Correleration  Weakness  Weakness_Min_Max  \n",
       "0      0.175282            0.015513  0.042072          0.198547  \n",
       "1      0.213565            0.065975  0.080199          1.000000  \n",
       "2      0.215133            0.147163  0.032627          0.000000  \n",
       "3      0.396020            0.225944  0.043202          0.222303  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将结果整合到DataFrame中\n",
    "calculation_results = pd.DataFrame({\n",
    "    'Topic': topic_docs.keys(),\n",
    "    'Docs': topic_docs.values(),\n",
    "    'Words': [list(words.iloc[row]) for row in range(len(words))],\n",
    "    'Ctfidfs': [list(ctfidfs.iloc[row]) for row in range(len(ctfidfs))],\n",
    "    'Closeness_Centrality': closeness_centrality.values(),\n",
    "    'Topic_Weight': topic_weights.values(),\n",
    "    'Auto_Correleration': topic_ac_scores.values()\n",
    "}).set_index('Topic')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#calculation_results[['Min_Max_CC','Min_Max_TW','Min_Max_AC']] = MinMaxScaler().fit_transform(calculation_results[['Closeness_Centrality', 'Topic_Weight', 'Auto_Correleration']])\n",
    "\n",
    "import math\n",
    "#calculation_results['Min_Max_Weakness'] = [(calculation_results.Min_Max_CC[row] * calculation_results.Min_Max_TW[row]) / (1 + math.exp(-calculation_results.Min_Max_AC[row])) for row in range(len(calculation_results))]\n",
    "calculation_results['Weakness'] = [(calculation_results.Closeness_Centrality[row] * calculation_results.Topic_Weight[row]) / (1 + math.exp(-calculation_results.Auto_Correleration[row])) for row in range(len(calculation_results))]\n",
    "\n",
    "calculation_results['Weakness_Min_Max'] = MinMaxScaler().fit_transform(calculation_results[['Weakness']])\n",
    "                                   \n",
    "calculation_results = calculation_results.sort_index().reset_index()\n",
    "\n",
    "calculation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "337ec583",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculation_results.to_csv('wsd_calculation_results_2018.csv', index=False)\n",
    "calculation_results.to_excel('wsd_calculation_results_2018.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1e863f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "没有弱主题。\n"
     ]
    }
   ],
   "source": [
    "weak_topics = [id for id in range(len(calculation_results)) if 0.01 <= calculation_results.Weakness_Min_Max[id] <= 0.1]\n",
    "if weak_topics:\n",
    "    print(f'弱主题有{weak_topics}。')\n",
    "else:\n",
    "    print('没有弱主题。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a038958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed_paper_id</th>\n",
       "      <th>citations_count</th>\n",
       "      <th>bert_abstract</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/W2785943049</td>\n",
       "      <td>40</td>\n",
       "      <td>study examine two important aspect late techno...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://openalex.org/W2790020274</td>\n",
       "      <td>31</td>\n",
       "      <td>advent electronic medical record ( emrs ) fuel...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://openalex.org/W2792048062</td>\n",
       "      <td>16</td>\n",
       "      <td>rise artificial intelligence recently lead bot...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://openalex.org/W2807786846</td>\n",
       "      <td>37</td>\n",
       "      <td>advent artificial intelligence way technology ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://openalex.org/W2809303504</td>\n",
       "      <td>1</td>\n",
       "      <td>artificial intelligence ( ai ) become primary ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://openalex.org/W2884382206</td>\n",
       "      <td>22</td>\n",
       "      <td>objective work present quick gbest guided arti...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://openalex.org/W2885623676</td>\n",
       "      <td>282</td>\n",
       "      <td>advances artificial intelligence ( ai ) transf...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://openalex.org/W2886383250</td>\n",
       "      <td>4</td>\n",
       "      <td>artificial neural networks ( anns ) genetic al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://openalex.org/W2888011899</td>\n",
       "      <td>6</td>\n",
       "      <td>development artificial intelligence technology...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://openalex.org/W2889038430</td>\n",
       "      <td>1</td>\n",
       "      <td>common sign development financing cryptocurren...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://openalex.org/W2890397306</td>\n",
       "      <td>13</td>\n",
       "      <td>fintech hot topic finance today recent advance...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://openalex.org/W2890456664</td>\n",
       "      <td>2</td>\n",
       "      <td>intensification globalization rapid developmen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://openalex.org/W2890642487</td>\n",
       "      <td>9</td>\n",
       "      <td>past decade cloud software transform numerous ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://openalex.org/W2890736909</td>\n",
       "      <td>5</td>\n",
       "      <td>return equity ( roe ) important factor perspec...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://openalex.org/W2891915267</td>\n",
       "      <td>24</td>\n",
       "      <td>financial revolution lead technology firm past...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://openalex.org/W2892259173</td>\n",
       "      <td>6</td>\n",
       "      <td>digital transformation ( dt ) inside higher ed...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://openalex.org/W2893353992</td>\n",
       "      <td>16</td>\n",
       "      <td>potential regulation use artificial intelligen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://openalex.org/W2894954870</td>\n",
       "      <td>1</td>\n",
       "      <td>relevance process sustainable use mineral reso...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://openalex.org/W2895317931</td>\n",
       "      <td>2</td>\n",
       "      <td>many author include schwab ( 2017 ) book ' fou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://openalex.org/W2900562530</td>\n",
       "      <td>8</td>\n",
       "      <td>cloud computing internet things ( iot ) big da...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://openalex.org/W2902397692</td>\n",
       "      <td>13</td>\n",
       "      <td>recent year artificial intelligence see increa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://openalex.org/W2903005174</td>\n",
       "      <td>95</td>\n",
       "      <td>specialty grand challenge article front artif ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://openalex.org/W2911746338</td>\n",
       "      <td>1</td>\n",
       "      <td>abstract purpose - purpose study analyze desig...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://openalex.org/W2912540451</td>\n",
       "      <td>18</td>\n",
       "      <td>paper provide decision - make debate support d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://openalex.org/W2942155101</td>\n",
       "      <td>3</td>\n",
       "      <td>involvement big populace quantitative trading ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://openalex.org/W2960192357</td>\n",
       "      <td>3</td>\n",
       "      <td>development science technology artificial inte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://openalex.org/W3022563550</td>\n",
       "      <td>4</td>\n",
       "      <td>study gaudily examine impact artificial intell...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>https://openalex.org/W3123286026</td>\n",
       "      <td>18</td>\n",
       "      <td>36 yale journal regulation 735 ( 2019).fintech...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://openalex.org/W3201689101</td>\n",
       "      <td>2</td>\n",
       "      <td>history research finance economics widely impa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>https://openalex.org/W4214481849</td>\n",
       "      <td>56</td>\n",
       "      <td>new landscape work new technology - digitaliza...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://openalex.org/W4232220395</td>\n",
       "      <td>14</td>\n",
       "      <td>article part debate david harvey michael hardt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>https://openalex.org/W4252124083</td>\n",
       "      <td>1</td>\n",
       "      <td>expert systems ( es ) core element human decis...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       seed_paper_id  citations_count  \\\n",
       "0   https://openalex.org/W2785943049               40   \n",
       "1   https://openalex.org/W2790020274               31   \n",
       "2   https://openalex.org/W2792048062               16   \n",
       "3   https://openalex.org/W2807786846               37   \n",
       "4   https://openalex.org/W2809303504                1   \n",
       "5   https://openalex.org/W2884382206               22   \n",
       "6   https://openalex.org/W2885623676              282   \n",
       "7   https://openalex.org/W2886383250                4   \n",
       "8   https://openalex.org/W2888011899                6   \n",
       "9   https://openalex.org/W2889038430                1   \n",
       "10  https://openalex.org/W2890397306               13   \n",
       "11  https://openalex.org/W2890456664                2   \n",
       "12  https://openalex.org/W2890642487                9   \n",
       "13  https://openalex.org/W2890736909                5   \n",
       "14  https://openalex.org/W2891915267               24   \n",
       "15  https://openalex.org/W2892259173                6   \n",
       "16  https://openalex.org/W2893353992               16   \n",
       "17  https://openalex.org/W2894954870                1   \n",
       "18  https://openalex.org/W2895317931                2   \n",
       "19  https://openalex.org/W2900562530                8   \n",
       "20  https://openalex.org/W2902397692               13   \n",
       "21  https://openalex.org/W2903005174               95   \n",
       "22  https://openalex.org/W2911746338                1   \n",
       "23  https://openalex.org/W2912540451               18   \n",
       "24  https://openalex.org/W2942155101                3   \n",
       "25  https://openalex.org/W2960192357                3   \n",
       "26  https://openalex.org/W3022563550                4   \n",
       "27  https://openalex.org/W3123286026               18   \n",
       "28  https://openalex.org/W3201689101                2   \n",
       "29  https://openalex.org/W4214481849               56   \n",
       "30  https://openalex.org/W4232220395               14   \n",
       "31  https://openalex.org/W4252124083                1   \n",
       "\n",
       "                                        bert_abstract  cluster  \n",
       "0   study examine two important aspect late techno...        2  \n",
       "1   advent electronic medical record ( emrs ) fuel...        3  \n",
       "2   rise artificial intelligence recently lead bot...        3  \n",
       "3   advent artificial intelligence way technology ...        1  \n",
       "4   artificial intelligence ( ai ) become primary ...        0  \n",
       "5   objective work present quick gbest guided arti...        2  \n",
       "6   advances artificial intelligence ( ai ) transf...        3  \n",
       "7   artificial neural networks ( anns ) genetic al...        0  \n",
       "8   development artificial intelligence technology...        2  \n",
       "9   common sign development financing cryptocurren...        2  \n",
       "10  fintech hot topic finance today recent advance...        2  \n",
       "11  intensification globalization rapid developmen...        0  \n",
       "12  past decade cloud software transform numerous ...        0  \n",
       "13  return equity ( roe ) important factor perspec...        2  \n",
       "14  financial revolution lead technology firm past...        0  \n",
       "15  digital transformation ( dt ) inside higher ed...        0  \n",
       "16  potential regulation use artificial intelligen...        1  \n",
       "17  relevance process sustainable use mineral reso...        3  \n",
       "18  many author include schwab ( 2017 ) book ' fou...        0  \n",
       "19  cloud computing internet things ( iot ) big da...        1  \n",
       "20  recent year artificial intelligence see increa...        1  \n",
       "21  specialty grand challenge article front artif ...        2  \n",
       "22  abstract purpose - purpose study analyze desig...        2  \n",
       "23  paper provide decision - make debate support d...        2  \n",
       "24  involvement big populace quantitative trading ...        2  \n",
       "25  development science technology artificial inte...        1  \n",
       "26  study gaudily examine impact artificial intell...        1  \n",
       "27  36 yale journal regulation 735 ( 2019).fintech...        2  \n",
       "28  history research finance economics widely impa...        1  \n",
       "29  new landscape work new technology - digitaliza...        1  \n",
       "30  article part debate david harvey michael hardt...        1  \n",
       "31  expert systems ( es ) core element human decis...        1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citations = pd.read_csv('citations_final_abstract_2018.csv')\n",
    "citations_count = citations.groupby('seed_paper_id').count()['id'].reset_index().rename(columns={'id':'citations_count'})\n",
    "seeds = pd.read_csv('seeds_final_abstract_2018.csv')\n",
    "citations_count['bert_abstract'] = [seeds.bert_final_abstract[list(seeds.id).index(id)] for id in citations_count.seed_paper_id]\n",
    "citations_count['cluster'] = [n for bert_abstract in citations_count.bert_abstract for n, docs in enumerate(calculation_results.Docs) if bert_abstract in docs]\n",
    "citations_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6eb64bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for weak_topic in weak_topics:\n",
    "    weak_signal_results = pd.DataFrame(data={'word': calculation_results.Words[weak_topic], 'ctfidf': calculation_results.Ctfidfs[weak_topic]})\n",
    "    weak_signal_results['min_max_ctfidf'] = MinMaxScaler().fit_transform(weak_signal_results[['ctfidf']])\n",
    "    weak_signal_results['weak_signal'] = [1 if 0.02 <= ctfidf <= 0.1 else 0 for ctfidf in weak_signal_results.min_max_ctfidf]\n",
    "    print(f'弱主题（主题{weak_topic}）有{weak_signal_results.weak_signal.sum()}个弱信号。')\n",
    "    weak_signals = [weak_signal_results.word[row] for row in range(len(weak_signal_results)) if weak_signal_results.weak_signal[row] == 1]\n",
    "    print(', '.join(weak_signals[:min(15,len(weak_signals))]) + '...')\n",
    "    weak_signal_results['citations_count'] = [sum([citations_count.citations_count[row] for row in citations_count[citations_count.cluster==weak_topic].index if word in citations_count.bert_abstract[row]]) for word in weak_signal_results.word]\n",
    "    weak_signal_filter_results = weak_signal_results[weak_signal_results.weak_signal==1].sort_values(by='citations_count', ascending=False).reset_index(drop=True)\n",
    "    weak_signal_filter_results['min_max_c_count'] = MinMaxScaler().fit_transform(weak_signal_filter_results[['citations_count']])\n",
    "    weak_signal_filter_results['weak_signal_filter'] = [1 if 0.02 <= c_count <= 0.1 else 0 for c_count in weak_signal_filter_results.min_max_c_count]\n",
    "    print(f'弱主题（主题{weak_topic}）有{weak_signal_filter_results.weak_signal_filter.sum()}个过滤后的弱信号。')\n",
    "    weak_signals_filter = [weak_signal_filter_results.word[row] for row in range(len(weak_signal_filter_results)) if weak_signal_filter_results.weak_signal_filter[row] == 1]\n",
    "    print(', '.join(weak_signals_filter))\n",
    "    weak_signal_filter_results.to_csv(f'wsd_filter_results_{weak_topic}_2018.csv',index=False)\n",
    "    weak_signal_filter_results.to_excel(f'wsd_filter_results_{weak_topic}_2018.xlsx',index=False)\n",
    "    with open(f'weak_signals_{weak_topic}_2018.txt', 'w') as f:\n",
    "        f.write(', '.join(weak_signals_filter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
