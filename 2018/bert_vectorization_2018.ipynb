{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "seeds = pd.read_csv('seeds_final_abstract_2018.csv')\n",
    "references = pd.read_csv('references_final_abstract_2018.csv')\n",
    "citations = pd.read_csv('citations_final_abstract_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "长度超过512的abstract数量: 6\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "# 找出 abstract 最长的行\n",
    "seeds['abstract_length'] = seeds['final_abstract'].apply(eval).apply(len)  # 计算每个 abstract 的单词数\n",
    "\n",
    "count_over_512 = (seeds['abstract_length'] > 512).sum()\n",
    "print(f\"长度超过512的abstract数量: {count_over_512}\")\n",
    "print(len(seeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'study examine two important aspect late technology issue islamic finance relate artificial intelligence ( ai ) smart contract ai refer ability machine understand think learn similar way human indicate possibility use computer simulate human intelligence smart contract computer code run top block - chain contain set rule party smart contract agree interact main objective article evaluate operation ai smart contract make comparison operation ai smart contract article conclude ai smart contract huge impact future islamic finance industry'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seeds.bert_final_abstract[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 创建有向图引用网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "# 创建有向图\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# 添加种子论文节点（红色）\n",
    "for _, row in seeds.iterrows():\n",
    "    G.add_node(row['id'], color='red', type='seed', title=row['title'], abstract=row['processed_abstract'], bert_abstract=row['bert_final_abstract'])\n",
    "\n",
    "# 添加参考文献节点（蓝色）和边\n",
    "for _, row in references.iterrows():\n",
    "    if row['id'] not in G:\n",
    "        G.add_node(row['id'], color='blue', type='reference', title=row['title'], abstract=row['processed_abstract'], bert_abstract=row['bert_final_abstract'])\n",
    "    G.add_edge(row['seed_paper_id'], row['id'])\n",
    "\n",
    "# 添加被引论文节点（绿色）和边\n",
    "for _, row in citations.iterrows():\n",
    "    if row['id'] not in G:\n",
    "        G.add_node(row['id'], color='green', type='citation', title=row['title'], abstract=row['processed_abstract'], bert_abstract=row['bert_final_abstract'])\n",
    "    G.add_edge(row['id'], row['seed_paper_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 文本向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 初始化BERT模型\n",
    "tokenizer = BertTokenizer.from_pretrained('../models/bert-base-uncased')\n",
    "model = BertModel.from_pretrained('../models/bert-base-uncased')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [15:41<00:00, 26.15s/it]\n"
     ]
    }
   ],
   "source": [
    "# 批量BERT向量化\n",
    "batch_size = 32\n",
    "nodes = list(G.nodes())\n",
    "for i in tqdm(range(0, len(nodes), batch_size)):\n",
    "    batch_nodes = nodes[i:i+batch_size]\n",
    "    texts = [(G.nodes[n]['bert_abstract']) for n in batch_nodes]\n",
    "    \n",
    "    inputs = tokenizer(texts, return_tensors='pt', \n",
    "                      truncation=True, max_length=512, \n",
    "                      padding='max_length').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    \n",
    "    for j, node in enumerate(batch_nodes):\n",
    "        G.nodes[node]['bert_vector'] = cls_embeddings[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 保存整个网络\n",
    "with open('bert_vectorized_network_2018.pkl', 'wb') as f:\n",
    "    pickle.dump(G, f)\n",
    "\n",
    "# 保存向量化模型(供后续使用)\n",
    "with open('bert_vectorizer_model_2018.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'bert_model': model.state_dict(),\n",
    "        'bert_tokenizer': tokenizer\n",
    "    }, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
